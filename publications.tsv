Timestamp	Your Name	Your personal website	Title	Authors	First author's last name	Year	URL	Download URL	Publication Details	Pages	Abstract	DOI	Context
03/26/2021 17:03:29	Legacy	https://web.archive.org	Automated Testing of Open-Source Music Software with Open Sound World and Open Sound Control	Amar Chaudhary	Amar Chaudhary	2005		files/Automated_Testing_of_Open-Source_Music_Software_with_Open_Sound_World_and_Open_Sound_Control.pdf	ICMC 2005		Providing robust systems for live musical performance has been a difficult problem. Such systems are highly dynamic with lots of paths for execution; even small changes in input can lead to different results. Comprehensive testing is difficult, tedious and resource intensive, more so for opensource projects that often lack the resources to do such testing. We present a system for efficient automated testing of the Open Sound World (OSW) open-source music environment using OpenSound Control (OSC) messages sent from Python scripts. The automated testing system helped the developers to release a significantly more robust version of OSW in 2004, and is an important tool in the development of OSW 2.0. OSW supports VST and LADSPA plug-ins as is binary compatible with most Pd externals. Thus, the automated testing system in OSW can be used to test these external plug-ins as well.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Best Practices for Open Sound Control	Andrew Schmeder, Adrian Freed, David Wessel	Schmeder	2010		files/osc-best-practices-final.pdf	Linux Audio Conference		The structure of the Open Sound Control (OSC) content format is introduced with historical context. The needs for temporal synchronization and dynamic range of audio control data are described in terms of accuracy, precision, bit-depth, bit-rate, and sampling frequency. Specific details are given for the case of instrumental gesture control, spatial audio control and synthesis algorithm control. The consideration of various transport mechanisms used with OSC is discussed for datagram, serial and isochronous modes. A summary of design approaches for describing audio control data is shown, and the case is argued that multi-layered information-rich representations that support multiple strategies for describing semantic structure are necessary.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Bidirectional XML mapping	Ben Chun	Chun	2004			OSC Conference 2004		We present a standard for bidirectional mapping of arbitrary OSC packets to and from XML documents. Such a mapping is already in use as part of the \"flosc\" implementation of OSC for Macromedia Flash; the purpose of the standard is to ensure that future uses of XML to represent OSC data will be consistent and mutually compatible. The nested structure of OSC bundles containing OSC messages containing type-tagged arguments is represented naturally by nested XML tags, as in this example: \[XXX example omitted\]		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Brief Overview of OSC and its Application Areas	Matt Wright	Wright	2004		files/wright-welcome.pdf	OSC Conference 2004				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Clock Synchronization for Interactive Music Systems	Roger Dannenberg	Dannenberg	2004		files/dannenberg-clocksync.pdf	OSC Conference 2004		Timestamped events allow computer networks to avoid some of the problems of network latency. Events are computed and delivered early to compensate for latency. Events are then scheduled using timestamps to compensate for variations in latency. This model assumes significant network latency, yet it relies on accurate local clocks. The problem of clock synchronization is to maintain a shared notion of global time with only local clocks and normal network communication. I will describe the typical drift properties of computer clocks (including digital audio sample clocks) and then describe several clock synchronization systems. While clock synchronization can be a low-level operating system facility, it is not difficult to build an application-layer clock synchronization facility on top of a communication mechanism such as OSC. I will describe how clock synchronization is integrated with audio synthesis and event scheduling in Aura.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Combining Audio And Gestures For A Real-Time Improviser	Roberto Morales-Mazanares, Eduardo Morales, David Wessel	Morales-Mazanares	2005	http://cnmat.berkeley.edu/publications/combining_audio_and_gestures_real_time_improviser	files/icmc05fin.pdf	International Computer Music Conference	813-816	Skilled improvisers are able to shape in real time a music discourse by continuously modulating pitch, rhythm, tempo and loudness to communicate high level information such as musical structures and emotion. Interaction between musicians, correspond to their cultural background, subjective reaction around the generated material and their capabilities to resolve in their own terms the aesthetics of the resultant pieces. In this paper we introduce GRI an environment, which incorporates music and movement gestures from an improviser to adquire precise data and react in a similar way as an improviser. GRI takes music samples from a particular improviser and learns a classifiers to identify different improvision styles. It then learns for each style a probabilistic transition automaton that considers gestures to predict the most probable next state of the musician. The current musical note, the predicted next state, and gesture information are used to produce adequate responses in real-time. The system is demonstrated with a flutist, with accelerometers and gyros to detect gestures with very promising results.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Comparing Musical Control Structures and Signal Processing Strategies for the Augmented Cello and Guitar	Adrian Freed, Ahm Lee, John Schott, Frances-Marie Uitti, Matthew Wright, Michael Zbyszynski	Freed	2006	http://cnmat.berkeley.edu/publications/comparing_musical_control_structures_and_signal_processing_strategies_augmented_cello_and_guitar		International Computer Music Conference	636-642	In this paper we examine the hardware (sensors and connectivity), physical performance (placement, sensor type, tactile feedback), and musical performance (software development and control) concerns inherent to extending control to six-string chordophones in general, and specifically to the electric guitar (plucked) and electric cello (bowed).		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Control of VST Plug-ins Using OSC	Michael Zbyszynski, Adrian Freed	Zbyszynski	2005	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/zbyszynski_ICMC3.pdf	International Computer Music Conference	263-266	The basic control structure of VST audio plug-ins can limit their usefulness. Control can be improved through the use of Open Sound Control by developing a flexible name space that employs multiple, intuitive parameter names (and aliases), higher-level controls and range mapping, simplifying control for the user. We will demonstrate these ideas with Max/MSP patches that repackage VST plug-ins in a more usable way and also introduce the idea that plug-in interfaces themselves can be improved by building in a well-formed OSC name space. Such a name space would enhance the longevity and flexibility of finished musical works. We will also show that when the plug-in is controlled directly with OSC atomicity and queries, control could be further improved.in		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Discovering OSC services with ZeroConf	Chandrasekar Ramakrishnan	Ramakrishnan	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/Rendezvous-OSC.pdf	OSC Conference 2004		ZeroConf/Rendezvous is a protocol for dynamic service discovery over a network, currently being standardized by the IETF. Through ZeroConf, applications and devices may communicate with each other without requiring prior manual configuration or setup. Music applications can take advantage of these capabilities to broadcast and discover the availability of services that communicate over OSC and, thereby, create ad-hoc music networks. Drawing on SuperCollider and Occam as examples, this talk will describe ZeroConf, discuss its benefits and use in the discovery of OSC services, and provide details and hints for programmers wishing to implement similar functionality in their own applications.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	Dynamic, Distributed and Automated Music Applications in Open Sound World	A Chaudhary	Chaudhary	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc		OSC Conference 2004		Open Sound World, or OSW, is a scalable, extensible programming environment that allows musicians, sound designers and researchers to process sound in response to expressive real-time control \[1\]. In OSW, components called transforms are combined to form programs called patches. Patches are themselves transforms that can be included in other patches. This gives rise to a hierarchical name space in which every transform has a unique address. This address can be used to send and receive OpenSound Control (OSC) messages to and from a transform. Additionally, the inlets, outlets and state variables (i.e., internal variables and parameters of a transform) are themselves named components with unique addresses. Thus an OSW program forms a complete OSC address space where any object (patch, transform, variable) can be queried inspected or modified using OSC messages. This presentation explores several applications that utilize OpenSound Control in OSW Discovery of OSW programs by generic clients: OSW includes a full implementation of OSC queries, which was largely the work of Andrew Schmeder at CNMAT \[3\]. Client programs can discover a running instance of an OSW by detecting a special â€œheartbeatâ€� message (i.e., similar to active sensing in MIDI), and then discover the dynamic address space of the OSW programs. The client can then use OSC to send any message understood by any of the objects in the OSW programs. Custom User Interfaces: The discovery/query system, as well as standard OSC messages can be used as a mechanism for inter-process communication between OSW and custom user interfaces. Such interfaces can be special front-ends for individual patches, or alternative user interfaces for building patches. We present specialized user interfaces as front-ends for patches that use the advanced synthesis capabilities of OSW. Distributed programs: In addition to communication between clients and servers, OSC can be used for peer communication among multiple instances of OSW. Such communication includes sending OSW time and signal data via OSC, allowing distributed synthesis and signal-processing on networked systems. Automated Testing: Building robust and reliable software is a challenging task, more so when the software must run on multiple platforms. For OSW, we have introduced a system of automated testing in which test cases are Python scripts that send and receive OSC messages that build, run, and inspect dynamically constructed patches. Using OpenSound Control as the â€œcommand languageâ€� for testing has allowed the rapid creation of test cases and uncovering of bugs that would otherwise be undetected until deployment. The test system as well as the fixes uncovered in the process will be part of the upcoming Open Sound World version 1.2 \[2\].		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:29	Legacy	https://web.archive.org	DySE Generator: A sound design tool for virtual reality applications	David Beaudry, Joan Slottow	Beaudry	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc		OSC Conference 2004		UCLA/ATS\'s DySE Generator (Dynamic Sound Environment) is a Mac-based software application that provides a powerful sound design tool for virtual reality (VR) applications. This software is built using Cycling74\'s Max/MSP/Jitter, a high-level graphical programming language for music, audio, and multimedia. DySE Generator had its origins as a custom-built tool for very specific applications controlling both live and pre-recorded sounds for live theater, but has since evolved into a less-specific tool for both building sound models, and functioning as a sound server, for virtual reality applications. Those familiar with Max know that one of its greatest advantages is its tremendous flexibility. It can do almost anything in real-time multimedia processing, and the fact that a patch can be built as needed for each application from the ground up, with every aspect of the sound design user-controllable, has made Max a solution that is almost without limits in the realm of sound design for live performance applications. However this lack of specialization is also a drawback in the deadline-driven world of sound design. Every time you sit down to create a new design in Max, you are starting with a blank screen. So rather than simply tell each sound designer \"Max is a great tool - now go learn it\", over the past three years we have been developing a wrapper that allows virtual reality sound designers and model builders (and by extension theater sound designers as well as sound designers for computer games) to use Max\'s real-time signal processing without having to program in Max. This has proven to be a formidable challenge: how to create a wrapper that is as powerful as Max, yet not be Max. Our process in developing DySE Generator has been to build functions in Max for very specific applications, then to make each working function into a non-specific \"tool\" with an easy-to-use GUI that is incorporated into DySE Generator. Since we view navigation of VR models as a form of performance, the primary motivation in DySE Generator\'s development is to focus more on a dynamic approach to the sonic material for virtual reality - what makes up the soundscape and source material of a particular model and how to make these sounds as engaging, dynamic, and performative for the viewer/observer as possible. For example, in addition to the more traditional functions of a sound server (i.e., playing sounds spatially while flying through a model), DySE Generator allows sound designers to establish relationships between human behavior and sound content by giving them the ability to map viewer behavior (e.g., how long they have remained in a particular room, directness of their path, total distance traveled, etc.) to control of the sonic material, allowing the designer to create a more personalized experience for each user. DySE Generator is more than simply a playback device or a sound server that focuses most of its computational power on modeling room acoustics; it is as an environment for specifying conditions and/or systems of relationships by which sounds will be produced and affected. Control of DySE Generator (either local or remote) uses OpenSoundControl (OSC) over UDP. For example, based on messages from the VR engine giving positional information for a particular sound source and viewer, DySE Generator is able to determine the panning and attenuation of the sound source and place it appropriately in the sound field. Here is the typical data flow: viewer behavior \> VR engine \> OSC (describing current state of model) \> DySE (as configured by sound designer) \> Max Patch\> Sound. Messages controlling many of the functions of the DySE Generator are embedded into vrNav, UCLA/ATSâ€™s navigation code for virtual reality. OSC messages can also go from DySE Generator back to vrNav, providing a two-way, network-based communication link between the sound server and virtual reality model that not only provides sound control, but also the ability to manipulate graphics in the virtual reality model from the sound server side. The OSC-based protocol has also been extended into other areas of research at ATS, including collaborative VR, lighting control in models using 3D Studio Max, and a new protocol for handing sonification of complex scientific data sets via the Sonification Toolbox, a module of the DySE Generator.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Effects of Latency on Networked Musical Performance	Michael Gurevich, Chris Chafe	Gurevich	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/gurevich-latency.pdf	OSC Conference 2004		The conditions of networked ensemble performance were simulated in an experiment. Pairs of musicians were placed apart in isolated rooms and given a simple rhythm to clap together. A microphone was placed as close as possible to each performer\'s hands. Each monitored the other\'s sound via headphones, and a delay was introduced between the source and listener. Starting tempo, given by a recorded count-in, and delay time were varied across trials. Recordings of the trials were analyzed with a precise event detection algorithm to locate clap onsets, from which the tempo was inferred. The rate of deceleration increased with longer delays, while shorter delays (\<= 11.5 ms) produced a modest, but significant acceleration. The goal is to identify the region of delay time that is most conducive to maintaining a steady tempo. This will help to determine the necessary delay conditions to support networked musical performance (which may be over long distances or in adjoining rooms). Humans performed significantly better than a simple model of a memoryless instantaneous reaction.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	EtherSense, an OSC-based sensor platform	Emmanuel FlÃ©ty, Nicolas Leroy	FlÃ©ty	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/ethersense-diagram.gif	OSC Conference 2004		This paper reports our recent developments on sensor acquisition systems, using computer network technology. We present a versatile hardware system that can be connected to wireless modules, Analog to Digital Converters, and enables Ethernet communication. We are planning to make freely available the design of this architecture. We describe also several approaches we tested for wireless communication. Such technology developments are currently used in our newly formed Performance Arts Technology Group.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Everything you ever wanted to know about Open Sound Control	Andy Schmeder	Schmeder	2008	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/Everything-About-OSC.mov	Report		Since its introduction in 1997 by CNMAT researchers Matt Wright and Adrian Freed, the Open Sound Control (OSC) protocol has been successfully integrated into dozens of hardware and software products, and used in thousands of performances and installations. OSC goes beyond MIDI by addressing needs specific to musical performance with new electronic instruments such as high-bandwidth connectivity, precise temporal semantics and temporal regularization, extensibility and rich type support, human readability and state-free operation. The OSC protocol itself has a simple structure making it easy to integrate into new applications, but an appreciation for the underlying design principles is necessary to leverage its full capabilities. This talk covers the basics of using and programming OSC with demonstrations of hardware and software implementations. We will then dive into the details and discuss best-practices for creating OSC implementations that relate to issues in musical control, with an emphasis on temporal semantics and related algorithms. The conclusion will include an overview of recent research and anticipated future developments.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Flash OSC	Ben Chun	Chun	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc		OSC Conference 2004		In the interest of providing a way to connect various audio synthesizers to Macromedia Flash for both visualization and interface development, flosc is a Java implementation of OSC that works over both TCP and UDP and uses an XML representation of OSC. Examples of the possibilities that this creates will be presented.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Future Directions for OSC	David Wessel, Matt Wright	Wessel	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/wright-future.pdf	OSC Conference 2004				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Human/Computer Interaction projects at CCRMA	Michael Gurevich	Gurevich	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc		OSC Conference 2004		Physical interaction design for music combines aspects of embedded systems, sensors, electronics, sound synthesis, design and HCI. CCRMA\'s courses in this area draw students with a variety of backgrounds in these fields, and expose them to aspects of each. Students learn about technology and design theory, from the instructors and from each other. Multidisciplinary team design projects to create physical interfaces for music and sound have resulted in a broad range of successful devices. The paradigmatic approach involves a device with sensors connected to a microcontroller, which digitizes, processes and encodes the signals before sending them to a host PC. Sound synthesis is done on the PC, based on the signals received. OpenSound Control and MIDI are employed and compared as communication protocols for gesture data.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Implementations of OSC	Matt Wright	Wright	2004	http://cnmat.berkeley.edu/publications/control_vst_plug_ins_using_osc	files/wright-implementations.pdf	OSC Conference 2004				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Improving the Efficiency of Open Sound Control with Compressed Address Strings	Jari Kleimola, Patrick McGlynn	Kleimola	2011	http://www.acoustics.hut.fi/publications/papers/smc2011-osc		Sound and Music Computing Conference (SMC-2011)	pp. 479-485	This paper introduces a technique that improves the efficiency of the Open Sound Control (OSC) communication protocol. The improvement is achieved by decoupling the user interface and the transmission layers of the protocol, thereby reducing the size of the transmitted data while simultaneously simplifying the receiving end parsing algorithm. The proposed method is fully compatible with the current OSC v1.1 specification. Three widely used OSC toolkits are modified so that existing applications are able to benefit from the improvement with minimal reimplementation efforts, and the practical applicability of the method is demonstrated using a multitouch-controlled audiovisual application. It was found that the required adjustments for the existing OSC toolkits and applications are minor, and that the intuitiveness of the OSC user interface layer is retained while communicating in a more efficient manner.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Intimate musical control of computers with a variety of controllers and gesture mapping metaphors	D Wessel, M Wright, J Schott	Wessel	2002	http://portal.acm.org/citation.cfm?id=1085213	files/NIME02WesselWrightSchottDmo.pdf	Proceedings of the 2002 conference on New interfaces for musical expression	1-3	In this demonstration we will show a variety of computer-based musical instruments designed for live performance. Our design criteria include initial ease of use coupled with a long term potential for virtuosity, minimal and low variance latency, and clear and simple strategies for programming the relationship between gesture and musical result. We present custom controllers and unique adaptations of standard gestural interfaces, a programmable connectivity processor, a communications protocol called Open Sound Control (OSC), and a variety of metaphors for musical control.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Keynote address: OSC and Digital Lifestyle Aggregation	Marc Canter	Canter	2004	http://portal.acm.org/citation.cfm?id=1085213		OSC Conference 2004		The history of media and device control parallels the evolution of the PC industry and of personal computing in general. Getting more control and ease of use in that process has always been a holy grail. But the oft-promised \'digital convergence\' has been elusive and end users have been bombarded by technology products that simply do not work. Digital Lifestyle Aggregation (\"DLA\") is a concept which brings together five major foundation areas, all of which lead to compelling end-user experiences. Personal publishing, social networking, media and device control, communications and mobile services are all part of the digital lifestyle aggregation puzzle. What does OSC and media device control have to do with DLA? A lot!		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	La Kitchen\'s \"Toaster\" and \"Kroonde\" wired and wireless Data-Acquisition Systems	Leslie Stuck	Stuck	2004	http://portal.acm.org/citation.cfm?id=1085213		OSC Conference 2004		The Kroonde is a system for receiving data from any industry-standard 0-5 volt sensor (e.g., pressure, flexion, acceleration, magnetic field strength, light intensity), sending the data via a small wireless transmitter to a central processor, and converting the data to either MIDI or OSC using UDP over 10 BaseT Ethernet. A demonstration will present strategies for using the data in a Max/MSP environment.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Max/MSP Programming Practice with OSC	David Wessel	Wessel	2004	http://cnmat.berkeley.edu/publications/max_msp_programming_practice_osc		Open Sound Control Conference				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	micro-OSC: The Open Sound Control Reference Implementation for Embedded Devices	Andrew Schmeder, Adrian Freed	Schmeder	2008	http://cnmat.berkeley.edu/publications/max_msp_programming_practice_osc	files/micro-OSC.mov	Report				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	Minuit : Propositions for a query system over OSC	Virage team	Virage team	2010	https://web.archive.org/web/20110122065602/http://www.plateforme-virage.org/?p=1444		Web Article PUBLICATION		The Minuit protocol has been designed in order to experiment a communication between the Virage sequencer software (a prototype dedicated to handle cues and automations on a Â« time-stretchable-line Â») and any device or sotfware (like controllers or any media engine). In order to allow users to select the parameter to store, update or observe, Minuit propose some specifications for a query system based on OSC. The protocol is based on four requests to discover an OSC namespace, to get or set values and to be notified when any of them changes.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:30	Legacy	https://web.archive.org	NIME 2006 OSC Developers Meeting	Matt Wright, RÃ©my Muller	Wright	2006	http://www.plateforme-virage.org/?p=1444		NIME 2006		Remy Muller (from IRCAM) and I organized an OSC Developers\' Meeting at the 2006 NIME conference (http://nime06.ircam.fr). The meeting was organized at the last minute and I profoundly apologize that nobody remembered to send out an invitation to the osc_dev list. Nevertheless, there were about 20 OSC developers in attendance.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Open Sound Control 1.0 Specification	Matthew Wright	Wright	2002	spec-1_0		Software PUBLICATION				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Open Sound Control: an enabling technology for musical networking	Matthew Wright	Wright	2005	http://cnmat.berkeley.edu/publications/open_sound_control_enabling_technology_musical_networking	files/S1355771805000932a.pdf	Organised Sound 10:3	193-200	Since telecommunication can never equal the richness of face-to-face interaction on itsown terms, the most interesting examples of networked music go beyond the paradigm of musicians playing together in a virtual room. The Open Sound Control protocol hasenabled dozens of interesting networked music projects. First the protocol itself isdescribed, followed by a representative list of some of the projects that the protocol has enabled.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Open Sound Control: State of the Art 2003	Matthew Wright, Adrian Freed, Ali Momeni	Wright	2003	http://cnmat.berkeley.edu/publications/open_sound_control_state_art_2003	files/Open+Sound+Control-state+of+the+art.pdf	International Conference on New Interfaces for Musical Expression	153-159	OpenSoundControl (?OSC?) is a protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. OSC has achieved wide use in the field of computer-based new interfaces for musical expression for wide-area and local-area networked distributed music systems, inter-process communication, and even within a single application.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	OSC and Gesture features of CNMAT\'s Connectivity Processor	Rimas Avizienis, Adrian Freed	Avizienis	2004	http://cnmat.berkeley.edu/publications/open_sound_control_state_art_2003	files/OSC2004-RimasBox.pdf	OSC Conference 2004		CNMAT\'s Connectivity Processor is a modular audio and gesture interface system based on a motherboard with connectors for two daughter cards. The motherboard provides digital audio connectivity using ADAT, AES-3 via Gigabit or 100BaseT ethernet to a host computer and additionally has one channel of MIDI in and out and 12 channels of D/A conversion. Typically one of the two daughter cards is used to provide analog audio A/D conversion and the other is used for multichannel gesture sensor acquisition. The primary advantages of this system over commercial Firewire audio devices are the reliable timing and delivery of gesture synchronized to the audio streams and the modularity of the daughterboard system. The first gesture sensor interface board we developed has 16 channels of analog input sampled with 12-bit resoluation at 1/8th the sample rate of the connectivity processor (48kHz or 44.1kHz). Each input has a 3rd-order lowpass analog filter and supports signal ranges from 0-5v. We chose the 3 row 48-pin DIN connector used by the ICube system to support a wide range of readily available sensors. For custom applications we also provide a DB25. The latest gesture sensor interface daughter board was developed to take advantage of an important trend in sensor design: integration of A/D conversion within the sensor. This board therefor has a modest 4-channels of A/D conversion and four DB-9 connectors for connecting digital sensor arrays using a new protocol under development at CNMAT. The DB-9 connector provides power and a configuration pin. All remaining pins are custom programmable using a Spartan III FPGA. The current OS/X driver for the connectivity processor upsamples sensor data to turn it into streams of audio giving access to the data with no jitter to any CoreAudio compatible application. MIDI data is routed through CoreMIDI but this interface doesn\'t appear to deliver the timing performance the hardware provides. We are now developing a driver to provide a UNIX socket interface to the gesture data which will be encoded as OSC messages. We preserve the timing performance by tagging the packets with tags derived from the precision sample rate clock of the connectivity processor.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	OSC Control of VST Plug-ins	Michael Zbyszynski, Adrian Freed	Zbyszynski	2004	http://cnmat.berkeley.edu/publications/open_sound_control_state_art_2003		OSC Conference 2004		While audio plug-ins are extremely useful, the limitations of the control structure can make that use unwieldy. Specifically, the name space of each VST plug-in is flat and populated by parameter names that have been carefully chosen by the designers of the plug-in, but do not necessarily represent the terminology or language preferred by the user. Parameter names are mapped through a generic range (0. to 1.) without informing the user about the mapping that occurs inside the plug-in, and each message controls only one parameter. Through the use of OSC, a flexible name space can be developed that employs multiple, intuitive parameter names (and aliases), higher level controls, and range mapping, simplifying control for the user. We will demonstrate these ideas with Max/MSP patches that repackage VST plug-ins in a more usable way and also introduce the idea that plug-in interfaces themselves can be improved by building in a well-formed OSC name space. We will also suggest ways (e.g., atomicity and queries) that control could be further improved if the plug-in could be controlled directly with OSC.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	OSC Design Space	Adrian Freed	Freed	2004	http://cnmat.berkeley.edu/publications/osc_design_space		Open Sound Control Conference				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	OSC Device Design Space	Adrian Freed	Freed	2004	http://cnmat.berkeley.edu/publications/osc_design_space	files/freed-devdesign.pdf	OSC Conference 2004		Most OSC packets flow between threads and processes on a general purpose computer or across networks between general purpose computers. In this presentation I will explore a growing new source of OSC packets: simple special purpose data acquisition and control systems. Other presentations in this session will describe several sophisticated OSC-communicating gesture sensor systems developed by relatively large institutions. To complement these I will focus on simpler, affordable options for individuals and small groups interested in developing their own OSC-communicating devices. The goal is to identify hardware platforms and development tools that are affordable, have sufficient performance to support OSC packetization and communication and require minimum development effort both to interface sensors and controllers and to develop the requisite software. Options explored include very cheap \"pic\" style microcontrollers, more sophisticated systems integrating 100BaseT networking, e.g. Rabbit 3000 series, I86-based systems supported by open source gnu tool chains, and a low cost USB interface device. I will emphasize the oft-forgotten importance of the development system and how the lowest cost device is rarely the best choice for installation, digital arts and musical applications where flexibility and a healthy performance margin lead to better artistic outcomes.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	An OSC Driver Framework for Gesture Sensors	Stephen PopeTravis	Pope	2004	http://cnmat.berkeley.edu/publications/osc_design_space	files/stp-create-osc.pdf	OSC Conference 2004		We have developed or integrated a number of gesture sensors over the past few years at CREATE, and have developed a software infrastructure for using them with OSC clients such as CSL and SuperCollider. This poster will outline the reusable software frameworks we use for the sensor server and synthesis client sides of our applications. Example code in C++ will be presented for the sensor-to-OSC and OSC-to-CSL interfaces.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	OSC Showcase for Maker Faire 2007	Adrian Freed, Andrew Schmeder, Michael Zbyszynski	Freed	2007	http://cnmat.berkeley.edu/publications/osc_design_space	files/OSC-Demo.pdf	Maker Faire 2007		This was an interactive demo and information booth at the 2007 Maker Faire featuring Open Sound Control hardware and software. Open Sound Control (OSC) is a protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. Compared to protocols such as MIDI, OSC\'s advantages include interoperability, accuracy, flexibility, and enhanced organization and documentation. ![](http://farm1.static.flickr.com/206/507425696_599591cb2b_o.jpg)		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Preparation for Interactive Live Computer Performance in Collaboration with a Symphony Orchestra	Timothy Madden, Ronald SmithBruce , Matthew Wright, David Wessel	Madden	2001	http://cnmat.berkeley.edu/publications/osc_design_space		ICMC 2001		This paper describes the design, implementation, and use of an interactive computer-based instrument in the context of a composition for orchestra and live electronics. We will begin with an overview of the piece from a compositional point of view, emphasizing the musical goals for the electronics aspect. Then we will discuss the extremely demanding technical requirements of integrating live electronics with a full orchestra. We will describe the interactive instrument in detail, emphasizing the novel mapping of performer\'s gestures to computer-generated sound events and the novel OpenSound Control-based structure of the software.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Problems and Prospects for Intimate Musical Control of Computers	David Wessel, Matthew Wright	Wessel	2002	http://mitpress.mit.edu/catalog/item/default.asp?ttype=6&tid=8962	files/p1-wessel_0.pdf	Computer Music Journal 26:3	11-22	In this demonstration we will show a variety of computer-based musical instruments designed for live performance. Our design criteria include initial ease of use coupled with a long term potential for virtuosity, minimal and low variance latency, and clear and simple strategies for programming the relationship between gesture and musical result. We present custom controllers and unique adaptations of standard gestural interfaces, a programmable connectivity processor, a communications protocol called Open Sound Control (OSC), and a variety of metaphors for musical control.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Problems and Prospects for Intimate Musical Control of Computers	David Wessel, Matthew Wright	Wessel	2001	http://cnmat.berkeley.edu/publications/problems_and_prospects_intimate_musical_control_computers	files/p1-wessel-1.pdf	ACM Computer-Human Interaction Workshop on New Interfaces for Musical Expression		In this paper we describe our efforts towards the development of live performance computer-based musical instrumentation. Our design criteria include initial ease of use coupled with a long term potential for virtuosity, minimal and low variance latency, and clear and simple strategies for programming the relationship between gesture and musical result. We present custom controllers and unique adaptations of standard gestural interfaces, a programmable connectivity processor, a communications protocol called Open Sound Control (OSC), and a variety of metaphors for musical control. We further describe applications of our technology to a variety of real musical performances and directions for future research.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Proceedings of The Open Sound Control Conference 2004	CNMAT	CNMAT	2004	2004-osc-conference		Open Sound Control Conference				This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	A Query System for Open Sound Control	Andrew Schmeder, Matthew Wright	Schmeder	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/osc-query-system.pdf
files/aws-query-cmd-system-osc04.pdf	OSC Conference 2004		A query system is proposed for inter-application control scenarios. The features consist of namespace exploration, documentation, type-signature, return-type-signature and parameter constraint specification, current-value polling, identification of common interpretation maps via osc-schema, as well as a general error reporting and reply mechanism.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:31	Legacy	https://web.archive.org	Quintet.Net: an interactive performance environment for the Internet	Georg Hajdu	Hajdu	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/quintet-dot-net.pdf	OSC Conference 2004		Quintet.net is an interactive network performance environment invented and developed by composer and computer musician Georg Hajdu. It enables performers at up to five locations to play music over the Internet under the control of a \"conductor.\" The environment, which was programmed with the graphical programming language Max/MSP, consists of five components: a Server, a Client, a Conductor, a Listener and a Viewer. These components exchange data using Matt Wrightâ€™s OpenSoundControl (OSC) and otudp Max external objects. Quintet.net was conceived in 1999, soon after CNMAT made these objects publicly available. The players interact over the Internet by sending control streams to the Server containing data that comes from a pitch tracker, MIDI, or simply the computer keyboard. On the Server, the streams are copied, processed, and sent back to the Clients as well as to the Listeners. In addition, a conductor can log onto the server and control the musical outcome by changing settings remotely and sending streams of parameter values as well as short instructions to the players. Quintet.net uses a sampler or MIDI for instrumental playback. It also features granular synthesis as well as VST plug-ins for sound processing and playback, and has additional video and graphical properties, which permit better interaction and control on a symbolic level: The performers along with the audience see the music which the participants produce on screen in \"space\" notation on five grand staves. In addition, video clips and/or live video can be displayed by the Viewer add-on and mixed with real-time music notation for an enhanced viewing experience. The Conductor can also read musical scores and send parts to the performers, which will be displayed along with the notes produced by the musicians. The music performed with Quintet.net is a combination of composed and improvised elements. The lack of real synchronicity (because of network latency) necessitates the adaptation of a genuine \"Internet\" performance style for which John Cage\'s number pieces could be considered a model: These pieces require certain notes or phrases to be played within \"time brackets.\" Recently, a composition development kit, including several visual editors, was added in order to facilitate the creation of pieces for the environment. Several pieces have specifically been written for the Quintet.net and successfully performed in high-profile venues.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	Real-time Distributed Media Applications in LANs with OSC	Tristan Jehan, Dan Overholt, Hugo GarciaSolÃ­s , Cati Vaucelle	Jehan	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/Tristan-2004-OSC-Poster.pdf	OSC Conference 2004		An increasing number of interactive applications must deal with real-time processing, and/or management of large databases of media (i.e., text, sound, images, videos). Often, limitations with computing power, requirements for distributing knowledge, availability and convenience of programming environments, make most intensive systems often fragmented across machines and even locations. These programs must exchange streaming data quickly, and seamlessly. The Media Laboratory explores and prototypes such systems. However, emphasis is often given on interactivity and content management rather than on communication technology. Open Sound Control (OSC) enables fast prototyping development of efficient communicating programs regardless of the programming language, computer architecture, and operating system. A series of systems using OSC in a local area network (LAN)â€"sometimes wirelessâ€"as main communication protocol were implemented and are presented. The applications include: real-time sound analysis of a symphony orchestra and synthesis of electronic sound masses; analysis and sound morphing of a singing voice with the â€œHyperviolinâ€�; SMS-ruled image database retrieval; high-dimensional and motion-controlled sound synthesis interfaces; a media jukebox; and an interactive online radio art installation. These applications use a wide range of environments and languages such as C++, Java, Max/MSP/Jitter, Supercollider, CSL, Flash, and Objective-C on Mac OS, Windows, or Linux platforms.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	Setting up OSC sessions using Voice-over-IP protocols	John Lazzaro	Lazzaro	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/lazzaro-osc-voip.mov	OSC Conference 2004		Performances are musical conversations. By extension, performances over a network can be viewed as musical conference calls. Ideally, we should be able to use the network protocols developed for voice-over-IP and video conferencing to do musical performance applications, by adding streams for musical gestural control to the audio and video streams. Thankfully, the network protocol suite developed by the Internet Engineering Task Force (IETF) for interactive media services is designed to make extensions of this sort easy to do. In the IETF stack, the Session Initiation Protocol (SIP) is used to do call management \-- to do the Internet equivalent of \"playing the dial-tone\" to the caller, letting the caller \"dial the phone\", play the \"busy signal\" or \"ringing tones\" to the caller, and \"putting the call through\" if the callee picks up the phone. Consumer services that use SIP today include Vonage and Apple\'s iChat AV. Behind the scenes, SIP (and helper protocols) solve messy problems. SIP lets phones negotiate the type of media to send, so that a videophone can take an audio-only call. SIP handles mobility, so a person with two offices can have phone calls follow him around the building. SIP also handles the modern realities of IP-life: user machines aren\'t always on-line, they may not have a stable domain name, they may not have a stable IP address, they might be behind a Network Address Translator, etc. And SIP can do so securely. SIP does not carry media flows itself, and is agnostic to what type of media is being used. Instead, SIP ferries \"session descriptions\" between clients, written in the Session Description Protocol (SDP). By extending SDP to handle a new media protocol (like OSC), one makes the new protocol SIP-compatible. In my talk, I\'ll describe the different ways that SDP could be extended to support OSC, and how the OSC community could work with the IETF to make it happen. As work in adding SDP support for MIDI 1.0 cable emulation (RTP MIDI) is already in progress in the IETF, adding OSC support to SDP would let SIP clients negotiate the use of OSC (if both clients were aware of each others OSC Address Space) or MIDI (if the clients were OSC-unaware) at the start of a call.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	SonART - a new multimedia environment for networked collaboration	Woon Yeo	Yeo	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control		OSC Conference 2004		SonART is a flexible, multi-purpose multimedia environment that allows for networked collaborative interaction with applications for art, science and industry. It provides an open-ended framework for integration of powerful image and audio processing methods with flexible network features. An arbitrary number of layered canvases, each with independent control of opacity, RGB values, image position, compositing options, etc., can transmit or receive data using OpenSoundControl. Data from images can be used for synthesis or audio signal processing and vice versa. Applications of SonART include multimedia art, collaborative and interactive art and design, and scientific and diagnostic exploration of data. Originally created for sound-driven data exploration and diagnostic purposes, the software also provides a powerful tool for multimedia performance over a network and facilitates real-time interactive creation, manipulation and exploration of audio and images. Now it is implemented as a Cocoa-based OS X application. Functionality includes the ability to load and draw multi-layered images, and to send/receive visual and control data to/from other multimedia programs that support OSC.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	Ten Years of Tablet Musical Interfaces at CNMAT	Michael Zbyszynski, Matthew Wright, Ali Momeni, Daniel Cullen	Zbyszynski	2007	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/Zbyszynski_NIME_CR02.pdf	New Interfaces for Musical Expression	100-105	We summarize a decade of musical projects and research employing Wacom digitizing tablets as musical controllers, discussing general implementation schemes using Max/MSP and OpenSoundControl, and specific implementations in musical improvisation, interactive sound installation, interactive multimedia performance, and as a compositional assistant. We examine two-handed sensing strategies and schemes for gestural mapping.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	Towards a More Effective OSC Time Tag Scheme	Adrian Freed	Freed	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/freed-timetags.pdf	OSC Conference 2004		Time tags are not widely used in OSC. This paper will introduce the original intention of time tags, discuss the challenges of implementing and using them, and introduce some new ideas about how they might be better employed and implemented. Time tags were introduced to address a number of problems and concerns we encountered at CNMAT using a predecessor protocol to OSC: 1) Synchronization of large parameter updates: What if the network transport imposes a limit on message size? How can you specify that a large number of OSC-addressable parameters are to be updated concurrently? The idea is that they can be broken up into a series of smaller OSC packets with the same time tag. 2) Synchronization when parameters are handled by different nodes on the network 3) Jitter Attenuation: by setting parameter updates to occur at a fixed interval in the future, jitter induced by networking and operating system delays can be eliminated. 4) Simplify implementation of sequencer applications. Programs simply buffer a few hundred milliseconds of OSC packets and hand them off for the OSC scheduler to do the fine grain timing Several important practical issues have prevented widespread adoption of time tags: 1) When OSC was first proposed, commercial operating systems did not come with compatible network time clients so there was no easy way to establish and interpret time tags using a central reliable timing source. 2) Many performances with OSC are in venues where access to the Internet is impossible and there are still no affordable, readily available NTP servers to provide a local master clock source. 3) The above history discouraged use of time tags and many implementations still ignore them, further discouraging their use. 4) With OSC it is the sender\'s responsibility to decide when a message will be processed by the receiver. Unfortunately it is the receiver that is in the best position to measure the communication latencies and no mechanism was included in the standard for the receiver to communicate this information to the sender. The sender doesn\'t know how far in the future to set the message processing to attenuate the jitter. The specification in OSC that clients and servers use the time tags that can all be interpreted as references from a single master clock is actually stronger than necessary in many applications. Jitter attenuation can be achieved by an adaptive scheme without requiring back channel communication using a stateless protocol. The idea is for senders to use their own 64-bit clock for constructing time tags. All messages are sent as a special bundle with a time tag that contains the time at which the sender sent the message. Receiving nodes maintain a histogram for each sender which measures the relative variations in received times. After receiving a certain number of packets the receiver can estimate the jitter statistics and derive a reasonable and slowly varying delay value that is used to rewrite the time tags in the body of the received packets conforming them to the receiver\'s clock. In OSC configurations supporting bidirectional communication, we can adopt a more complicated scheme to establish the actual value of the communication latency. The idea is that receivers return small packets which contain their idea of the current time back to senders. Using the same techniques used in NTP, the senders can adjust their clocks to be close to the receivers\' clocks. This scheme is more robust than reliance on a central NTP server as it supports senders and receivers leaving and joining the network dynamically, a common requirement in OSC-based collaborative performances, during debugging and on wireless networks.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	TUIO: A protocol for table-top tangible user interfaces	M Kaltenbrunner, T Bovermann, R Bencina, E Costanza	Kaltenbrunner	2005	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/tuio_gw2005.pdf	Proc. of the The 6th International Workshop on Gesture in Human-Computer Interaction and Simulation		In this article we present Tuio, a simple yet versatile protocol designed specifically to meet the requirements of table-top tangible user interfaces. Inspired by the idea of interconnecting various existing table interfaces such as the reacTable\*, being developed in Barcelona and the tDesk from Bielefeld, this protocol defines common properties of controller objects on the table surface as well as of finger and hand gestures performed by the user. Currently this protocol has been implemented within a fiducial marker-based computer vision engine developed for the reacTable\* project. This fast and robust computer vision engine is based on the original d-touch concept, which is also included as an alternative to the newer fiducial tracking engine. The computer vision framework has been implemented on various standard platforms and can be extended with additional sensor components. We are currently working on the tracking of finger-tips for gestural control within the table interface. The Tuio protocol has been implemented using OpenSound Control and is therefore usable on any platform supporting this protocol. At the moment we have working implementations for Java, C++, PureData, Max/MSP, SuperCollider and Flash.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
03/26/2021 17:03:32	Legacy	https://web.archive.org	Use of Open Sound Control in SuperCollider Server	J McCartney	McCartney	2004	http://cnmat.berkeley.edu/publications/query_system_open_sound_control	files/OSC_in_SC_Server.pdf	OSC Conference 2004		The SuperCollider audio synthesis environment consists of a synthesis server and a programming language client which communicate via Open Sound Control (OSC). The SuperCollider server was designed to be the simplest possible implementation of a dynamically reconfigurable synthesis engine. The design metaphor was that it would be a virtual machine for audio with OSC commands as the virtual machine instructions. Like a virtual machine, the synthesis engine is divided into a few major functional units (buses, buffers, and execution tree) and the OSC commands operate on these units. Because synthesis nodes can appear and disappear quite rapidly, the usual implementation of an OSC address space where each node in a tree is named was not practical and was not used. Instead the command space is flat and nodes are accessed by number. Some unique features, such as using OSC blobs to embed completion messages for asynchronous operations, will be discussed and some proposals for discussion for new capabilities will be put forth.		This was a featured publication on the legacy (pre-2011) opensoundcontrol.org website, ported to the new site by Matt Wright in early 2021
4/17/2021 9:39:01	Matt Wright	https://ccrma.stanford.edu/~matt	Open SoundControl: A New Protocol for Communicating with Sound Synthesizers	Matthew Wright, Adrian Freed	Wright	1997	http://hdl.handle.net/2027/spo.bbp2372.1997.033	files/1997-ICMC-OSC.pdf	International Computer Music Conference (ICMC)	101-104	Open SoundControl is a new protocol for communication among computers, sound synthesizers, and other multimedia devices that is optimized for modern networking technology. Entities within a system are addressed individually by an open-ended URL-style symbolic naming scheme that includes a powerful pattern matching language to specify multiple recipients of a single message. We provide high resolution time tags and a mechanism for specifying groups of messages whose effects are to occur simultaneously. There is also a mechanism for dynamically querying an Open SoundControl system to find out its capabilities and documentation of its features.		This paper is how Wright/Freed/CNMAT first shared OSC with the world.  Some aspirational aspects of this 1997 vision of OSC are still not fully realized (e.g., the query system), and some important aspects of OSC did not appear here (e.g., the type tag system).
4/23/2021 10:16:00	Matt Wright	https://ccrma.stanford.edu/~matt	Implementation and Performance Issues with OpenSound Control	Matthew Wright	Wright	1998	http://hdl.handle.net/2027/spo.bbp2372.1998.281	files/1998-OSC-Kit.pdf	International Computer Music Conference (ICMC)	224-227	OpenSound Control (OSC) is a new protocol for high-level, expressive control of sound synthesis and other multimedia applications. It includes time-tagged messages, guaranteed atomicity of messages with the same time tag, and regular expression address patterns that can match multiple messages in a receiving application. We hope OSC becomes a standard, and towards this end have made available at no cost the OpenSound Control Kit, a C or C++ library that adds OSC addressability to an application. This Kit is designed to be added to a sound processing application without degrading reactive real-time performance. This paper accompanies the Kit and provides a high- level overview of the Kit's interfaces to the rest of an application and how performance issues are addressed.		This was the second OSC paper, based on the premise that OSC's adoption required an open-source implementation of OSC's features; it embodies Matt's then-somewhat-naive assumptions about what potential OSC users might do and how to help them. The described code (the "OSC Kit") did actually exist and did pass some of its own tests, but as far as we know was *never* incorporated into any real-time sound processing software as designed. Parts of it (such as the byte format parser and the recursive descent pattern matcher) were borrowed for other implementations. Eventually newer and better-focused OSC libraries such as [liblo](../implementations/Liblo.html) and [oscpack](../implementations/oscpack.html) replaced the idea of a need for the OSC Kit.
4/28/2021 10:16:00	Matt Wright	https://ccrma.stanford.edu/~matt	Managing Complexity with Explicit Mapping of Gestures to Sound Control with OSC	Matthew Wright, Adrian Freed, Ahm Lee, Tim Madden, Ali Momeni	Wright	2001	http://hdl.handle.net/2027/spo.bbp2372.2001.087	files/2001-ICMC-Managing-Complexity-OSC.pdf	International Computer Music Conference (ICMC)	314-317	We present a novel use of the OpenSound Control (OSC) protocol to represent the output of gestural controllers as well as the input to sound synthesis processes. With this scheme, the problem of mapping gestural input into sound synthesis control becomes a simple translation from OSC messages into other OSC messages. We provide examples of this strategy and show benefits including increased encapsulation and program clarity.		This paper describes several CNMAT projects' use of OSC not just as a technical solution to transfer messages among devices but more generally as a way to represent the data at all stages of the transformation from physical measurements of musicians' movement into digitally sculpted sound synthesis. Adrian Freed's 2021 [annotated version](http://opensoundcontrol.org/OSCGestureMapping) adds much more context.
