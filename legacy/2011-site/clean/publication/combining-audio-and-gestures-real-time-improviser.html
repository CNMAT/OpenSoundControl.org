<h2>
 Combining Audio And Gestures For A Real-Time Improviser
</h2>
<div class="content">
 <div id="biblio-node">
  <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&rfr_id=info%3Asid%2Fopensoundcontrol.org&rft.title=Combining+Audio+And+Gestures+For+A+Real-Time+Improviser&rft.date=2005&rft.spage=813&rft.epage=816&rft.aulast=Morales-Mazanares&rft.aufirst=Roberto&rft.au=Morales%2C+Eduardo&rft.au=Wessel%2C+David&rft.pub=International+Computer+Music+Association&rft.place=Barcelona%2C+Spain&rft_id=http%3A%2F%2Fcnmat.berkeley.edu%2Fpublications%2Fcombining_audio_and_gestures_real_time_improviser">
  </span>
  <table>
   <tbody>
    <tr class="odd">
     <td align="right">
      <span class="biblio-row-title">
       Publication Type
      </span>
     </td>
     <td>
     </td>
     <td>
      Conference Proceedings
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Year of Publication
      </span>
     </td>
     <td>
     </td>
     <td>
      2005
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Authors
      </span>
     </td>
     <td>
     </td>
     <td>
      <a href="publications/author/Morales-Mazanares">
       Morales-Mazanares, Roberto
      </a>
      ;
      <a href="publications/author/Morales">
       Morales, Eduardo
      </a>
      ;
      <a href="publications/author/Wessel">
       Wessel, David
      </a>
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Conference Name
      </span>
     </td>
     <td>
     </td>
     <td>
      International Computer Music Conference
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Pagination
      </span>
     </td>
     <td>
     </td>
     <td>
      813-816
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Publisher
      </span>
     </td>
     <td>
     </td>
     <td>
      International Computer Music Association
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Conference Location
      </span>
     </td>
     <td>
     </td>
     <td>
      Barcelona, Spain
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Abstract
      </span>
     </td>
     <td>
     </td>
     <td>
      Skilled improvisers are able to shape in real time a music discourse by continuously modulating pitch, rhythm, tempo and loudness to communicate high level information such as musical structures and emotion. Interaction between musicians, correspond to their cultural background, subjective reaction around the generated material and their capabilities to resolve in their own terms the aesthetics of   the resultant pieces. In this paper we introduce GRI an environment, which incorporates music and movement gestures from an improviser to adquire precise data and react in a similar way as an improviser. GRI takes music samples from a particular improviser and learns a classifiers to identify different improvision styles. It then learns for each style a probabilistic transition automaton that considers gestures to predict the most probable next state of the musician. The current musical note, the predicted next state, and gesture information are used to produce adequate responses in real-time. The system is demonstrated with a flutist, with accelerometers and gyros to detect gestures with very promising results.
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       URL
      </span>
     </td>
     <td>
     </td>
     <td>
      <a href="http://cnmat.berkeley.edu/publications/combining_audio_and_gestures_real_time_improviser">
       http://cnmat.berkeley.edu/publications/combining_audio_and_gestures_real_time_improviser
      </a>
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
      </span>
     </td>
     <td>
     </td>
     <td>
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="center" width="20%">
      <span class="biblio-row-title">
       Export
      </span>
     </td>
     <td>
     </td>
     <td>
      <a href="publications/export/tagged/171">
       EndNote Tagged
      </a>
      |
      <a href="publications/export/xml/171">
       XML
      </a>
      |
      <a href="publications/export/bib/171">
       BibTex
      </a>
     </td>
    </tr>
   </tbody>
  </table>
 </div>
 <table id="attachments">
  <thead>
   <tr>
    <th>
     Attachment
    </th>
    <th>
     Size
    </th>
   </tr>
  </thead>
  <tbody>
   <tr class="odd">
    <td>
     <a href="files/icmc05fin.pdf">
      icmc05fin.pdf
     </a>
    </td>
    <td>
     65.16 KB
    </td>
   </tr>
  </tbody>
 </table>
</div>
