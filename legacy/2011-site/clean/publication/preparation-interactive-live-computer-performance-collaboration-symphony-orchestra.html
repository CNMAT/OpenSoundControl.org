<h2>
 Preparation for Interactive Live Computer Performance in Collaboration with a Symphony Orchestra
</h2>
<div class="content">
 <div id="biblio-node">
  <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&rfr_id=info%3Asid%2Fopensoundcontrol.org&rft.title=Preparation+for+Interactive+Live+Computer+Performance+in+Collaboration+with+a+Symphony+Orchestra&rft.date=2001&rft.aulast=Madden&rft.aufirst=Timothy&rft.au=Smith%2C+Ronald+Bruce&rft.au=Wright%2C+Matthew&rft.au=Wessel%2C+David&rft.place=Habana%2C+Cuba">
  </span>
  <table>
   <tbody>
    <tr class="odd">
     <td align="right">
      <span class="biblio-row-title">
       Publication Type
      </span>
     </td>
     <td>
     </td>
     <td>
      Conference Paper
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Year of Publication
      </span>
     </td>
     <td>
     </td>
     <td>
      2001
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Authors
      </span>
     </td>
     <td>
     </td>
     <td>
      <a href="publications/author/Madden">
       Madden, Timothy
      </a>
      ;
      <a href="publications/author/Smith">
       Smith, Ronald Bruce
      </a>
      ;
      <a href="publications/author/Wright">
       Wright, Matthew
      </a>
      ;
      <a href="publications/author/Wessel">
       Wessel, David
      </a>
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Conference Name
      </span>
     </td>
     <td>
     </td>
     <td>
      ICMC 2001
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Conference Location
      </span>
     </td>
     <td>
     </td>
     <td>
      Habana, Cuba
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
       Abstract
      </span>
     </td>
     <td>
     </td>
     <td>
      This paper describes the design, implementation, and use of 
an interactive computer-based instrument in the context of a 
composition for orchestra and live electronics. We will 
begin with an overview of the piece from a compositional 
point of view, emphasizing the musical goals for the 
electronics aspect. Then we will discuss the extremely 
demanding technical requirements of integrating live 
electronics with a full orchestra. We will describe the 
interactive instrument in detail, emphasizing the novel 
mapping of performer's gestures to computer-generated 
sound events and the novel OpenSound Control-based 
structure of the software.
     </td>
    </tr>
    <tr class="odd">
     <td align="right" valign="top" width="20%">
      <span class="biblio-row-title">
      </span>
     </td>
     <td>
     </td>
     <td>
     </td>
    </tr>
    <tr class="even">
     <td align="right" valign="center" width="20%">
      <span class="biblio-row-title">
       Export
      </span>
     </td>
     <td>
     </td>
     <td>
      <a href="publications/export/tagged/186">
       EndNote Tagged
      </a>
      |
      <a href="publications/export/xml/186">
       XML
      </a>
      |
      <a href="publications/export/bib/186">
       BibTex
      </a>
     </td>
    </tr>
   </tbody>
  </table>
 </div>
 <form>
  <fieldset class="collapsible collapsed">
   <legend>
    Full-Text
   </legend>
   <div>
    <p align="CENTER">
     Timothy Madden, Ronald Bruce Smith, 
Matthew Wright, & David Wessel
    </p>
    <p align="CENTER">
     CNMAT, UC Berkeley, 1750 Arch St., Berkeley, CA, 94709
    </p>
    <h3>
     Abstract
    </h3>
    <p>
     This paper describes the design, 
implementation, and use of an interactive computer-based instrument in the context 
of a composition for orchestra and live electronics. We will begin with an overview 
of the piece from a compositional point of view, emphasizing the musical goals 
for the electronics aspect. Then we will discuss the extremely demanding technical 
requirements of integrating live electronics with a full orchestra. We will describe 
the interactive instrument in detail, emphasizing the novel mapping of performer's 
gestures to computer-generated sound events and the novel OpenSound Control-based 
structure of the software.
    </p>
    <h3>
     1. Introduction
    </h3>
    <p>
     This paper describes 
the design, implementation, and use of an interactive computer-based instrument 
for Ronald Bruce Smith's composition
     <i>
      Constellation
     </i>
     for orchestra and live 
electronics.
     <i>
      Constellation
     </i>
     was premiered on November 10, 2000 by the Berkeley 
Symphony Orchestra conducted by Kent Nagano. The concert was held in Hertz Hall 
on the UC Berkeley campus.
    </p>
    <h3>
     2. Musical Goals
    </h3>
    <p>
     Harmonic and inharmonic 
spectra (from bell-like spectra) form the basis of the harmony in
     <i>
      Constellation
     </i>
     . 
From this, several layers of harmony and spectral reinforcement were composed. 
First, spectral components were subjectively selected by the composer, resynthesized 
using additive synthesis and orchestrated using acoustic orchestral instruments. 
Secondly, certain pitches (components) from a given spectrum were selected as 
principal melodic tones. Those pitches were then highlighted through both the 
orchestra and electronics by reinforcing selected spectral components of the harmonic 
spectrum of the orchestral instrument(s) performing the selected pitch(es) of 
the resynthesized timbre.
    </p>
    <p>
     With respect to harmonic movement in the composition, 
interpolations between spectra were calculated using IRCAM's Patchwork software 
. The interpolations were then subjectively altered by the composer, especially 
with respect to the voicings of the harmonies that the interpolations yielded. 
Those harmonies were then resynthesized to form timbres that were subsequently 
treated by the layering and reinforcement techniques outlined above.
    </p>
    <p>
     The 
electronic instrument in this piece was used to produce several kinds of sounds. 
Some of the inharmonic spectra described above were synthesized directly with 
additive synthesis. Other inharmonic spectra, such as percussive bell-like tones, 
were synthesized using exponentially decaying sinusoids. Similar synthesis techniques 
produced glockenspiel-like tones. Many of the notes in the orchestral parts were 
doubled by harmonic, synthetic timbres, also produced with additive synthesis. 
Other parts used sample playback.
    </p>
    <h3>
     3. Implementation
    </h3>
    <h4>
     3.1	Goals
    </h4>
    <p>
     The 
implementation of this instrument was determined in large part by the special 
and severe constraints of working with a symphony orchestra. We refused to freeze 
the electronics part on tape and require the conductor to wear headphones with 
a click track; instead we made all of the electronics performable by a musician 
sitting in the orchestra watching the conductor. Rehearsal time was extremely 
limited, so it would have been a disaster to have to wait for a computer to reboot; 
therefore high reliability was essential. Limited rehearsal time and the lack 
of any sound check give us no time to adjust the volumes of the electronic parts 
in context with the orchestra, so we made all gains controllable in real-time 
by both the performer and the composer sitting in the hall. The conductor needed 
to be able to jump from section to section in rehearsal, so the electronics needed 
to be able to quickly go to any event in the score.
    </p>
    <h4>
     3.2	Hardware
    </h4>
    <p>
     <img src="https://web.archive.org/web/20161220164754im_/http://www.cnmat.berkeley.edu/ICMC2001/html/constellation-stage-layout.gif">
    </p>
    <p align="CENTER">
     Figure 
1. In this schematic diagram of the concert set up, the audio connections are 
not shown. They run between the multi-channel DAC’s on each of the computers 
and the audio mixer. The audio mixer was configured to switch its source rapidly 
and silently from one computer to the other if one failed. Likewise, the MIDI 
switcher was available to redirect MIDI in the event of computer failure. The 
composer controlled the gains of the audio sources from his position in the center 
of the audience with the motorized faders of a MotorMix Controller. During rehearsals 
the gains were adjusted and set. During the performance they were fine-tuned by 
overriding the settings sent from the computers to the MotorMix. The note keyboard 
was equipped with a pedal for tapping in the tempo given by the conductor.
    </p>
    <p>
    </p>
    <p>
     We 
used the Max/MSP environment for all of the software for this piece. The entire 
program ran redundantly on two Macintosh G3 computers, so that if one crashed 
during a rehearsal or performance, we could instantly switch to using the other. 
The performer played two MIDI keyboards, including volume pedal, pitch bend wheel, 
and sustain pedal. The composer sat in the hall with a CM Automation MotorMix 
motorized fader bank, finely adjusting the levels and the envelopes of the synthesized 
timbres during rehearsal and performance. We used MOTU 2408 sound I/O hardware 
with each Macintosh, both connected to the speakers via a Mackie 1604 mixer. Synthetic 
sound was diffused from an array of six Meyer UPL-1 loudspeakers situated in a 
horseshoe shape around and behind the orchestra. Figure 1 shows the physical layout 
of this equipment for the performance.
    </p>
    <h4>
     3.3 Software
    </h4>
    <p>
     The two MIDI 
keyboards played by the live musician had very different effects on the Max/MSP 
patch. One of the keyboards, called the "event keyboard," was used to 
set the software into states to synthesize specific sounds. For example, if a 
section of the score called for a bell tone, the musician pressed a specific key 
on the event keyboard to trigger an event, in this case, the synthesis of a bell 
sound. The Max/MSP patch is essentially stateless except for whatever was configured 
by the most-recently-played key on the event keyboard; in other words, each new 
event supercedes the previous event and does not rely on previous events having 
been played. The use of the event keyboard made possible the ability to move effortlessly 
and swiftly to any section of the score during a rehearsal since the computer 
could be set to any state at any time.
    </p>
    <p>
     The second keyboard, called the 
"note keyboard," was played like a traditional piano, with each key 
triggering a note at a given pitch. The note keyboard can play different timbres 
at different times, thanks to events that reconfigure the mapping of the note 
keyboard.
    </p>
    <p>
     Unique to the software was the use of Open Sound Control (OSC) 
to control all aspects of the patch. All of the sound synthesis capabilities of 
the software were arranged in an OSC address space. For example, to play the "glock" 
instrument at some pitch at some loudness, the OSC command is /glock/playnote
     <i>
      note loudness
     </i>
     . Table 1 lists the entire OSC address space for this patch. 
The only way for the patch to make a sound is for the control part of the patch 
to send an OSC message to the synthesis part of the patch.
    </p>
    <blockquote>
     <blockquote>
      <p>
       /ScoreEvent/midinote-off
       <br>
       /ScoreEvent/midinote-on
       <br>
       /bell1/cutoff-freq
       <br>
       /glockies/midinote-off
       <br>
       /glockies/midinote-on
       <br>
       /harmtones/osc[1-8]/glissdir
       <br>
       /harmtones/osc[1-8]/glissmag
       <br>
       /harmtones/osc[1-8]/init
       <br>
       /harmtones/osc[1-8]/mute
       <br>
       /harmtones/osc[1-8]/play
       <br>
       /harmtones/osc[1-8]/volume-pedal
       <br>
       /midi-routing/note-gate
       <br>
       /midi-routing/note-gate/glockies
       <br>
       /midi-routing/note-gate/ronbell
       <br>
       /midi-routing/note-gate/sampler
       <br>
       /midi-routing/volume-pedal-gate
       <br>
       /midi-routing/volume-pedal-gate/bell1-freq
       <br>
       /midi-routing/volume-pedal-gate/harmtone-volume
       <br>
       /ronbells/mute
       <br>
       /ronbells/play/bell2a
       <br>
       /ronbells/play/bell2b
       <br>
       /ronbells/play/bell2c
       <br>
       /ronbells/play/bell2d
       <br>
       /ronbells/play/bell3a
       <br>
       /ronbells/play/bell3b
       <br>
       /sampler/midinote-on
       <br>
       /sampler/midinote-off
       <br>
       /sampler/multisample
       <br>
       /sampler/speaker
       <br>
       /sampler/master-volume
       <br>
       <i>
        <br>
        Table 
1: OSC Address Space of the Patch
       </i>
      </p>
     </blockquote>
    </blockquote>
    <p>
     All OSC 
messages went through a central OSC dispatcher that routes each message to the 
correct place. This gave us an extremely useful debugging tool: seeing every message 
the patch was sending itself. The typical software architecture of a large Max/MSP 
patch, with multiple send and receive objects, makes it very difficult to trace 
all of the intra-patch communication.
    </p>
    <p>
     Since any event in the score (each 
corresponding to a note on the event keyboard) requires several simultaneous messages 
to be sent to the patch, an electronic score or database of OSC commands was used 
to store these messages. In this way, when a note on the event keyboard was pressed, 
several OSC commands were issued from the electronic score. We used Max’s
     <font face="Courier" size="2">
      coll
     </font>
     object to store the sequence of OSC commands 
corresponding to each event. Table 2 shows the collection of OSC messages that 
implement event number two in the electronics score. The first event disables 
glissandi for all of the additive-synthesis-generated "harmtones". The 
second event remaps the MIDI volume pedal to control the volume of the harmtones. 
The 3
     <sup>
      rd
     </sup>
     and 4
     <sup>
      th
     </sup>
     assign the output of the first four harmtone 
voices to particular speakers; note the use of OSC pattern matching. The 5
     <sup>
      th
     </sup>
     through 8
     <sup>
      th
     </sup>
     events cause the harmtones to synthesize tones with given 
frequencies and loudnesses.
    </p>
    <blockquote>
     <blockquote>
      <p>
       1, /harmtones/osc*/glissdir 
0.;
       <br>
       2, /midi-routing/volume-pedal-gate/harmtone-volume 1;
       <br>
       3, /harmtones/osc{1\,4}/speaker/number 
2;
       <br>
       4, /harmtones/osc[2-3]/speaker/number 5;
       <br>
       5, /harmtones/osc1/play 1760. 
2.3;
       <br>
       6, /harmtones/osc2/play 3078.530762 2.3;
       <br>
       7, /harmtones/osc3/play 3960.205811 
2.3;
       <br>
       8, /harmtones/osc4/play 4841.90625 2.3;
       <br>
       <br>
       <i>
        Table 2: OSC Commands 
Comprising Score Event 2
       </i>
      </p>
     </blockquote>
    </blockquote>
    <h4>
     3.4	Synthesis
    </h4>
    <p>
     Because 
the computer must synthesize a large variety of sounds throughout the piece, including 
bell-like tones, harmonic tones, glockenspiel-like sounds, and various percussive 
sounds, several synthesis methods were employed. The glockenspiel-like sounds 
were synthesized with digital resonators (using the
     <font face="Courier" size="2">
      resonators~
     </font>
     MSP object) from analysis data of a marimba sound stored in Sound Description 
Interchange Format (SDIF) . To avoid simply reproducing transpositions of the 
marimba sound, the sound was filtered to add many irregularly-spaced formants 
to the sound. Also, randomness was added to the spectral envelope to make each 
attack sound slightly different. To prevent high notes from sounding too metallic, 
the decay time was inversely scaled with fundamental frequency so that high notes 
decay faster than low notes. In order to make the timbre change with loudness, 
a brightness model based on exponential functions was employed.
    </p>
    <p>
     Harmonic 
tones were produced with additive synthesis using MSP's
     <font face="Courier" size="2">
      sinusoids~
     </font>
     object . Small time-varying deviations 
were added to the partials to enrich the sound. Also, variance in odd/even partial 
content, formants, and exponential brightness models were used.
    </p>
    <p>
     Bell tones 
were synthesized with digital resonators using predefined lists of frequencies, 
amplitudes, and decay rates. The decay rate can be altered in real time using 
the MotorMix.
    </p>
    <p>
     Finally, the patch created some percussive sounds using samples. 
In some instances, prerecorded MIDI sequences were used to play long passages 
of notes. In order to keep the tempo of the MIDI sequence synchronized with the 
live orchestra conductor, the keyboardist tapped a foot pedal in a tempo established 
by the conductor thus setting the speed of the MIDI sequence. A novel MIDI sequencer, 
allowing real-time tempo changes, was created for this purpose.
    </p>
    <p>
     The MotorMix, 
used by the composer to adjust the loudness of the electronic sounds, features 
motorized faders which can be controlled by the Max patch to store a preset mix. 
A problem when storing a preset mix is synchronizing the mix with the live performance. 
To solve this problem, a database was created to store the MotorMix settings. 
The event keyboard, which triggers state changes in the patch, was used to reference 
the database, and thus control the MotorMix. During the rehearsal, the composer 
could adjust the faders on the MotorMix and store the settings in the database 
indexed by the note played on the event keyboard. In this way, the mix could be 
synchronized to the live performance.
    </p>
    <h3>
     4. Conclusions
    </h3>
    <p>
     We now detail 
the successes and failures of the design of the electronics for this piece. The 
use of the event keyboard to control the electronic sounds proved to be a success. 
The written electronics part played by a live musician resembled a piano score, 
allowing a professional pianist to play the electronics part with relatively little 
rehearsal time. A slight failure was that there is no current standard for the 
layout of the conductor's score when live electronics is involved. During the 
rehearsals, the conductor was a bit uncomfortable with location of the electronics 
part which was placed in the written score where one would notate a keyboard part. 
We found that it is best to keep the standard orchestra layout of all of the acoustic 
parts, and add electronic parts to the top or bottom of the written score.
    </p>
    <p>
     The 
Max patch was created by musically knowledgeable engineers in close collaboration 
with the composer. The composer described the desired sound, the engineers designed 
software to approximate the composer's desires, and the process would iterate 
until both the engineers and composer were satisfied. The process of close collaboration 
seemed to be successful.
    </p>
    <p>
     The use of OpenSound Control was a success in that 
it offered a simple unified interface to control the patch. In orchestra rehearsal, 
several unforeseen problems with the Max patch arose, requiring modifications 
of the patch during rehearsal time. The OpenSound Control interface allowed us 
to solve unforeseen problems without wasting valuable rehearsal time.
    </p>
    <h3>
     References
    </h3>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     Assayag, 
G. 1993. CAO: Vers la partition potentielle. Paris: IRCAM.
    </p>
    <p>
     Jehan, T., A. 
Freed, and R. Dudas 1999. Musical Applications of New Filter Extensions to Max/MSP. 
Proceedings of the ICMC, Beijing, China.
    </p>
    <p>
     Laurson, M. and J. Duthen 1990. 
A compositional environment based on Preform II, Patchwork and Esquisse. Proceedings 
of the International Computer Music Conference.
    </p>
    <p>
     Schwarz, D. and M. Wright 
2000. Extensions and Applications of the SDIF Sound Description Interchange Format. 
Proceedings of the International Computer Music Conference, Berlin, Germany, pp. 
481-484.
    </p>
    <p>
     Smith, R. B. 2000. "Constellation for orchestra and live 
electronics," Canadian Music Centre, 20 St. Joseph Street, Toronto, ON, M4Y 
1J9 Canada.
    </p>
    <p>
     Wright, M., A. Chaudhary, A. Freed, S. Khoury, and D. Wessel 
1999. Audio Applications of the Sound Description Interchange Format Standard. 
Proceedings of the Audio Engineering Society 107th Convention. (http://cnmat.CNMAT.Berkeley.EDU/AES99/docs/AES99-SDIF.pdf)
    </p>
    <p>
     Wright, 
M. and A. Freed 1997. Open Sound Control: A New Protocol for Communicating with 
Sound Synthesizers. Proceedings of the International Computer Music Conference, 
Thessaloniki, Hellas, pp. 101-104.
    </p>
    <p>
     Wright, M., A. Freed, A. Lee, T. Madden, 
and A. Momeni 2001. Managing Complexity with Explicit Mapping of Gestures to Sound 
Control with OSC. Proceedings of the International Computer Music Conference, 
Habana, Cuba.
    </p>
    <p>
     Zicarelli, D. 1998. An Extensible Real-Time Signal Processing 
Environment for Max. Proceedings of the International Computer Music Conference, 
Ann Arbor, Michigan, pp. 463-466.
    </p>
   </div>
  </fieldset>
 </form>
</div>
