::: {.content}
::: {#biblio-node}
[ ]{.Z3988
title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adc&rfr_id=info%3Asid%2Fopensoundcontrol.org&rft.title=Human%2FComputer+Interaction+projects+at+CCRMA&rft.date=2004&rft.aulast=Gurevich&rft.aufirst=Michael"}

  ---------------------------------------------- -- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
         [ Publication Type ]{.biblio-row-title}    Conference Paper
      [ Year of Publication ]{.biblio-row-title}    2004
                  [ Authors ]{.biblio-row-title}    [Gurevich, Michael](publications/author/Gurevich)
          [ Conference Name ]{.biblio-row-title}    OSC Conference 2004
    [ Conference Start Date ]{.biblio-row-title}    30/07/2004
                 [ Abstract ]{.biblio-row-title}    Physical interaction design for music combines aspects of embedded systems, sensors, electronics, sound synthesis, design and HCI. CCRMA\'s courses in this area draw students with a variety of backgrounds in these fields, and expose them to aspects of each. Students learn about technology and design theory, from the instructors and from each other. Multidisciplinary team design projects to create physical interfaces for music and sound have resulted in a broad range of successful devices. The paradigmatic approach involves a device with sensors connected to a microcontroller, which digitizes, processes and encodes the signals before sending them to a host PC. Sound synthesis is done on the PC, based on the signals received. OpenSound Control and MIDI are employed and compared as communication protocols for gesture data.
                          [ ]{.biblio-row-title}    
                   [ Export ]{.biblio-row-title}    [EndNote Tagged](publications/export/tagged/194) \| [XML](publications/export/xml/194) \| [BibTex](publications/export/bib/194)
  ---------------------------------------------- -- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
:::
:::
